{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86557797-36de-44bb-b277-2ee7a22b1458",
   "metadata": {},
   "source": [
    "# ðŸ‘‹ Getting started 1: Creating a 1-Lipschitz neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5c863-6aad-4741-aaaa-71ad9f719f8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23c0071a-2546-4143-af7e-496d9ffa2fd4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The goal of this series of tutorials is to show the different usages of `deel-lip`.\n",
    "\n",
    "In this first notebook, our objective is to show how to create 1-Lipschitz neural networks with `deel-lip`. \n",
    "\n",
    "In particular, we will cover the following: \n",
    "1. [ðŸ“š Theoretical background](#theoretical_background)    \n",
    "A brief theoretical background on Lipschitz continuous functions. This section can be safely skipped if one is not interested in the theory.\n",
    "2. [ðŸ§± Creating a 1-Lipschitz neural network with `deel-lip` and `keras`](#deel_keras)       \n",
    "An example of how to create a 1-Lipschitz neural network with `deel-lip` and `keras`.\n",
    "3. [ðŸ”¨ Design rules for 1-Lipschitz neural networks with `deel-lip`](#design)   \n",
    "A set of neural network design rules that one must respect in order to enforce the 1-Lipschitz constraint.\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ“š Theoretical background <a id='theoretical_background'></a> <a name='theoretical_background'></a>\n",
    "### What is a Lipschitz constant\n",
    "The `deel-lip` package allows to control the Lipschitz constant of a layer or of a whole neural network. The Lipschitz constant is a mathematical property of a function (in our context of work, a layer or a model) that characterizes how much the output of the function can change with respect to changes in its input. \n",
    "\n",
    "In mathematical terms, a function $f$ is Lipschitz continuous with a **Lipschitz constant K** or more simply **K-Lipschitz** if for any given pair of points $x_1,x_2$, $K$ provides a bound on the rate of change of $f$:  \n",
    "\n",
    "$$||f(x_1)-f(x_2)||\\leq K||x_1-x_2||.$$\n",
    "\n",
    "For instance, given a 1-Lipschitz dense layer (a.k.a fully connected layer) with a weight matrix $W$ and a bias vector $b$, we have for any two inputs $x_1$ and $x_2$: $$||W.x_1+b-(W.x_2+b)|| \\leq 1||x_1-x_2||.$$\n",
    "\n",
    "ðŸ’¡ The norm we refer to throughout our notebooks is the Euclidean norm (L2). This is because `deel-lip` operates with this norm. You will find more information about the role of the norm in the context of adversarially robust 1-Lipschitz deep learning models in the notebook titled 'Getting Started 2'.\n",
    "\n",
    "### A simple requirement for creating 1-Lipschitz neural network\n",
    "The composition property of Lipschitz continuous functions states that if you have a function f that is $K_1$-Lipschitz and another function g that is $K_2$-Lispchitz, then their composition function h = (f o g) which applies f after g is also Lipschitz continuous with a Lipschitz constant $K \\leq K_1$ * $K_2$.\n",
    "\n",
    "A neural network is essentially a stack of layers that transform the output of the layer before them and whose output is fed to the layer after them. \n",
    "\n",
    "By the composition property of Lipschitz functions, *it suffices for each of the n individual layers of a neural network model to be 1-Lipschitz, for the whole model to be 1-Lipschitz*.\n",
    "\n",
    "For instance, given a 1-Lipschitz dense layer parametrized by $(W_1,b_1)$, and a ReLU (Rectified Linear Unit) activation layer which is naturally 1-Lipschitz, the combination of the two is also 1-Lispchitz.   \n",
    "This is shown in the equations below, where we have for any two inputs $x_1$ and $x_2$:\n",
    "\n",
    "$$||W_1.x_1+b_1-(W_1.x_2+b_1)||\\leq 1||x_1-x_2||,$$\n",
    "$$||ReLU(x_1)-ReLU(x_2)||\\leq 1||x_1-x_2||,$$\n",
    "and:\n",
    "$$||ReLU(W_1.x_1+b_1)-ReLU(W_1.x_2+b_1)||\\leq 1||W_1.x_1+b_1-(W_1.x_2+b_1)||\\leq 1^2||x_1-x_2||.$$\n",
    "\n",
    "\n",
    "The `deel-lip` package allows to create 1-Lipschitz neural networks, by providing the user with means to enforce the Lipschitz constant 1 on a selected set of layers (such as dense layers).   \n",
    "It also ensures that 1-Lipschitz continuity is retained while the model is being trained by managing the changes to trainable parameters.\n",
    "\n",
    "\n",
    "## ðŸ§± Creating a 1-Lipschitz neural network with `deel-lip` and `keras` <a id='deel_keras'></a> <a name='deel_keras'></a>\n",
    "`keras` is an open-source high-level deep learning API written in Python. It allows to build, train, and deploy deep learning models.\n",
    "\n",
    "One can produce a neural network architecture using keras with a few lines of code, as shown in the toy-example multi-layer perceptron (MLP) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3f0694-8547-4d06-b2aa-bfc0d008ff8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52650 (205.66 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "num_classes=10\n",
    "\n",
    "# a basic model that does not follow any Lipschitz constraint\n",
    "model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "model.compile(optimizer='adam',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b53d4-5585-41d9-a301-0012c54dba0b",
   "metadata": {},
   "source": [
    "Alternatively, it is equivalent to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f7099e-a425-452d-9ed5-5328f0258a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52650 (205.66 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "x = keras.layers.Flatten()(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "y = layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f72425",
   "metadata": {},
   "source": [
    "`deel-lip` extends `keras`' capabilities by introducing custom `layers` and `model` modules, to provide the ability to control the Lipschitz constant of layers objects or of complete neural networks, while keeping a user-friendly interface.\n",
    "\n",
    "Below is a 1-Lipschitz replication of the previous MLP toy-example, using `deel-lip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a62d3a3f-c0e8-4a3e-9025-758afebf99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deel\n",
    "from deel import lip\n",
    "\n",
    "activation=lip.activations.GroupSort2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26f52a6-c2ec-49b5-a99f-3353dc3f3044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " spectral_dense_3 (Spectral  (None, 64)                100481    \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      " spectral_dense_4 (Spectral  (None, 32)                4161      \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      " spectral_dense_5 (Spectral  (None, 10)                661       \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105303 (411.34 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 52653 (205.68 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K1_model = lip.model.Sequential([    \n",
    "        keras.layers.Input(shape=input_shape),\n",
    "        keras.layers.Flatten(),\n",
    "        lip.layers.SpectralDense(64, activation=activation()),\n",
    "        lip.layers.SpectralDense(32, activation=activation()),\n",
    "        lip.layers.SpectralDense(num_classes, activation=None)\n",
    "    ],\n",
    "    \n",
    "    k_coef_lip=1,\n",
    "\n",
    ")\n",
    "\n",
    "K1_model.compile(optimizer='adam',\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "K1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daaddb4",
   "metadata": {},
   "source": [
    "Alternatively, it is equivalent to write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a98e31f-40a9-46f5-a4e6-bd91229046e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kierszbaums\\anaconda.related\\envs\\1_lipschitz\\deel_lip\\lib\\site-packages\\keras\\src\\initializers\\initializers.py:120: UserWarning: The initializer Orthogonal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " spectral_dense (SpectralDe  (None, 64)                100481    \n",
      " nse)                                                            \n",
      "                                                                 \n",
      " spectral_dense_1 (Spectral  (None, 32)                4161      \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      " spectral_dense_2 (Spectral  (None, 10)                661       \n",
      " Dense)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 105303 (411.34 KB)\n",
      "Trainable params: 52650 (205.66 KB)\n",
      "Non-trainable params: 52653 (205.68 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "x = keras.layers.Flatten()(inputs)\n",
    "x = lip.layers.SpectralDense(64, activation=activation(),k_coef_lip=1.)(x)\n",
    "x = lip.layers.SpectralDense(32, activation=activation(),k_coef_lip=1.)(x)\n",
    "y = lip.layers.SpectralDense(num_classes, activation=None,k_coef_lip=1.)(x)\n",
    "K1_model = lip.model.Model(inputs=inputs, outputs=y)\n",
    "K1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded0081",
   "metadata": {},
   "source": [
    "To summarize, there exists 2 ways to specify the Lipschitz constant of a model to 1:\n",
    "- by specifying the Lispchitz constant of each of its layer through their `k_coef_lip` attribute, when using a `lip.model.Model` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae5d775-fb90-4017-919d-bd34c08865cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(input_shape)\n",
    "x = keras.layers.Flatten()(inputs)\n",
    "# k_coef_lip sets the Lipschitz constant of the layer. Its value is 1 by default.\n",
    "x = lip.layers.SpectralDense(64, activation=activation(),k_coef_lip=1.)(inputs)\n",
    "x = lip.layers.SpectralDense(32, activation=activation(),k_coef_lip=1.)(x)\n",
    "y = lip.layers.SpectralDense(num_classes, activation=None,k_coef_lip=1.)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ff2460d-b1f3-43d6-964d-5919b24009e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "K1_model = lip.model.Model(inputs=inputs, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38b069-1705-408c-8dd3-99ed17cf519f",
   "metadata": {},
   "source": [
    "- by specifying the Lipschitz constant of the whole model through the `k_coef_lip` attribute of a `Sequential` object, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb882d3e-163f-4aa8-902f-83922ac8da89",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1507601557.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    ],\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "K1_model = lip.model.Sequential([    \n",
    "        ....\n",
    "    ],\n",
    "    # This parameter sets the Lipschitz constant of the whole model. Its value is 1 by default.\n",
    "    k_coef_lip=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8046c-115a-4fea-b36e-16b05a811897",
   "metadata": {},
   "source": [
    "ðŸ’¡\n",
    "Keep in mind that all the classes above inherit from their respective `keras` equivalent (e.g. `Dense` for `SpectralDense`).   <br>\n",
    "As a result, these objects conveniently use the same interface and the same parameters as their keras equivalent, with the additional parameter `k_coef_lip` that controls the Lipschitz constant.\n",
    "\n",
    "## ðŸ”¨ Design rules for 1-Lipschitz neural networks with `deel-lip`  <a id='design'></a> <a name='design'></a>\n",
    "**Layer selection: `deel-lip` vs `keras`**  \n",
    "<br/> \n",
    "In our 1-Lipschitz MLP examples above, we have used a mixture of objects from both `keras` and `deel-lip` `layers` submodule (e.g. the `Input` layer for `keras`, the `SpectralDense` layer for `deel-lip`).\n",
    "\n",
    "More generally, for the particular types of layers that do not interfere with the Lipschitz property of any neural network they belong to, no alternative has been coded in `deel-lip` and the existing `keras` layer object can be used. \n",
    "\n",
    "This is the case for the following keras layers: `MaxPooling`, `GlobalMaxPooling`, `Flatten` and `Input`.\n",
    "\n",
    "Below is the full list of `keras` layers for which `deel-lip` provides a Lipschitz equivalent. If one wants to ensure a model's Lipschitz continuity, the alternative `deel-lip` layers must be employed instead of the original `keras` counterparts.\n",
    "\n",
    "| tensorflow.keras.layers | deel.lip.layers |\n",
    "| --------------- | --------------- |\n",
    "| `Dense`    | `SpectralDense`<br>|\n",
    "| `Conv2D`   | `SpectralConv2D`<br>  |\n",
    "|  `AveragePooling2D`<br>`GlobalAveragePooling2D` | `ScaledAveragePooling2D`<br>`ScaledGlobalAveragePooling2D`|\n",
    "\n",
    "<br/>\n",
    "\n",
    "ðŸ’¡ Although there are additional Lipschitz continuous layers available in `deel-lip`, the ones mentioned above are perfectly suitable and recommended for practical use. Interested readers can find information about the other layers [here](#documentation).\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "ðŸš¨ **Note:** *When creating a 1-Lipschitz neural network, one should avoid using the following layers:*<br> \n",
    "- `Dropout`: Our current recommendation is to avoid using it, as we have not yet fully understood how it affects learning of 1-Lipschitz neural networks\n",
    "- `BatchNormalization`: It is not 1-Lipschitz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f0f4f-16de-4172-a4e9-b4798f0bd678",
   "metadata": {},
   "source": [
    "\n",
    "**Activation function selection:**\n",
    "\n",
    "The ReLU and softmax activation functions are both Lipschitz continuous with a Lipschtiz constant of 1. \n",
    "\n",
    "However, as can be seen in our examples, the following is perfectly suitable and recommended for practical use:\n",
    "- using the `GroupSort2` activation function stored in the `activations` submodule of `deel-lip` for the intermediate layers of a 1-Lipschitz neural network.\n",
    "- not using any activation function for the last layer of 1-Lipschitz neural networks.\n",
    "\n",
    "ðŸ’¡ Interested readers can find information relevant to other 1-Lipschitz activation functions that exist within `deel-lip` [here](https://deel-ai.github.io/deel-lip/api/layers/).\n",
    "\n",
    "\n",
    "**Loss function selection:**\n",
    "\n",
    "One can use `keras` loss functions to train 1-Lipschitz neural networks. Doing so will not interfere with the 1-Lipschitz continuity of the model.  \n",
    "\n",
    "ðŸ’¡ `deel-lip` also has a `losses` submodule that contains several loss functions. They are parametrized to let the user create adversarially robust functions, which is not the case with `keras` loss functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4857464-3ed3-436e-8853-9ed67d0ce5c6",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations\n",
    "You now know how to create 1-Lipschitz neural networks!\n",
    "\n",
    "In the next tutorial, we will see how to train and assess adversarially robust 1-Lipschitz neural networks on the classification task, using `deel-lip`'s `losses` submodule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
