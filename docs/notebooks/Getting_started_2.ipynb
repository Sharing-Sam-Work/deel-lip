{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86557797-36de-44bb-b277-2ee7a22b1458",
   "metadata": {},
   "source": [
    "# ðŸ‘‹ Getting started 2: Training adversarially robust 1-Lipschitz neural networks for classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23c0071a-2546-4143-af7e-496d9ffa2fd4",
   "metadata": {},
   "source": [
    "The goal of this series of tutorials is to show the different usages of `deel-lip`.\n",
    "\n",
    "In this first notebook, we have shown how to create 1-Lipschitz neural networks with `deel-lip`.    \n",
    "In this second notebook, we will show how to train adversarially robust 1-Lipschitz neural networks with `deel-lip`.    \n",
    "\n",
    "In the course of this notebook, we will cover the following: \n",
    "1. [ðŸ“š Theoretical background](#theoretical_background)    \n",
    "A brief theoretical background on adversarial robustness. This section can be safely skipped if one is not interested in the theory.\n",
    "2. [ðŸ’ª Training adversarially robust 1-Lipschitz neural networks on the MNIST dataset](#deel_keras)       \n",
    "Using the MNIST dataset, we will show examples of training adversarially robust 1-Lipschitz neural networks using `deel-lip` loss functions `TauCategoricalCrossentropy` and `MulticlassHKR`.\n",
    "\n",
    "We will also see that:\n",
    "- the `MulticlassKR` loss function can be used to assess the adversarial robustness of the resulting models\n",
    "- when training robust models, there is an accuracy-robustness trade-off.\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ“š Theoretical background <a id='theoretical_background'></a>\n",
    "### Adversarial attacks\n",
    "In the context of classification problems, an adversarial attack is the result of adding an *adversarial perturbation* $\\epsilon$ to the input data point $x$ of a trained predictive model $A$, with the intent to change its prediction.\n",
    "\n",
    "In simple mathematical terms, an adversarial example (i.e. a succesful adversarial attack) can be transcribed as below:\n",
    "\n",
    "$$A(x)=y_1,$$\n",
    "$$A(x+\\epsilon)=y_{\\epsilon},$$\n",
    "where:\n",
    "$$y_1\\neq y_\\epsilon.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8769d17-2c47-4491-ad22-7785324f88ec",
   "metadata": {},
   "source": [
    "### An adversarial example\n",
    "\n",
    "The following example is directly taken from https://adversarial-ml-tutorial.org/introduction/.\n",
    "\n",
    "![pigs.png](../assets/pigs.png)\n",
    "\n",
    "The first image is correctly classified as a **pig** by a classifier. The second image is incorrectly classified as an **airplane** by the same classifier. \n",
    "\n",
    "While both images cannot be distinguished from our (human) perspective, the second image is in fact the result of surimposing \"noise\" (i.e. adding an adversarial perturbation) to the original first image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e3cb53-98f7-44cb-bb9b-29b94e4fc6bd",
   "metadata": {},
   "source": [
    "Below is a visualization of the added noise, zoomed-in by a factor of 50 so that we can see it:\n",
    "![noise.png](../assets/noise.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4125128-de5d-4267-83ae-892e9d8ced69",
   "metadata": {},
   "source": [
    "### Adversarial robustness of 1-Lipschitz neural network\n",
    "The adversarial robustness of a predictive model is its ability to remain accurate and reliable when subjected to adversarial perturbations.  \n",
    "\n",
    "Practically, training an adversarial robust predictive model consists in obtaining a formal guarantee that it remains accurate, even when exposed against adversarial attacks that are defined within certain specified constraints.\n",
    "\n",
    "A major advantage of 1-Lipschitz neural networks is that they can offer provable guarantees on their robustness for any particular input $x$.\n",
    "\n",
    "One can formulate such a guarantee, in terms such as follow:\n",
    "\n",
    "> \"For an input $x$, we can certify that are no adversarial perturbations constrained to be under the certificate $\\epsilon_x$ that will change our model's prediction.\"\n",
    "\n",
    "In simple mathematical terms:  \n",
    "\n",
    "For a given $x$, $\\forall \\epsilon$ such that $||\\epsilon||<\\epsilon_x$, we obtain that:\n",
    "$$A(x)=y,$$\n",
    "$$A(x+\\epsilon)=y_{\\epsilon},$$\n",
    "then:\n",
    "$$y_{\\epsilon}=y.$$\n",
    "\n",
    "ðŸ’¡ The *certificate* $\\epsilon_x$ attached to an input $x$ can be deduced from the logits of the 1-Lipschitz neural network used. This will be the topic of another notebook. In this notebook, robustness will be approximated by the `MulticlassKR` loss, as shown later.\n",
    "\n",
    "ðŸ’¡ Depending on the type of norm you choose (e.g. L1 or L2), the guarantee you can offer will differ, as $||\\epsilon||_{L2}<\\epsilon_x$ and $||\\epsilon||_{L1}<\\epsilon_x$ are not equivalent.\n",
    "\n",
    "As such, other examples of guarantees with a more precise formulation would be:\n",
    "> \"For an input $x$, we can certify that are no adversarial perturbations constrained to be within a $\\text{L2}$-norm ball of certificate $\\epsilon_{x,\\text{L2}}$ that will change our model's prediction.\"\n",
    "\n",
    "> \"For an input $x$, we can certify that are no adversarial perturbations constrained to be within a $\\text{L1}$-norm ball of certificate $\\epsilon_{x,\\text{L1}}$ that will change our model's prediction.\"\n",
    "\n",
    "\n",
    "\n",
    "## ðŸ’ª Training adversarially robust 1-Lipschitz neural networks on the MNIST dataset <a id='deel_keras'></a>\n",
    "\n",
    "\n",
    "### ðŸ’¾ MNIST dataset\n",
    "MNIST dataset contains a large number of 28x28 digit images to which are associated digit labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cf25c6-1691-4935-bdac-9f182a87ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90da93e3-9c59-4e72-b817-01f97b324c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Database\n",
    "(X_train, y_train_ord), (X_test, y_test_ord) = mnist.load_data()\n",
    "\n",
    "# standardize and reshape the data\n",
    "X_train = np.expand_dims(X_train, -1) / 255\n",
    "X_test = np.expand_dims(X_test, -1) / 255\n",
    "\n",
    "# one hot encode the labels\n",
    "y_train = to_categorical(y_train_ord)\n",
    "y_test = to_categorical(y_test_ord)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b52bdd-6cca-487e-8a37-82c4436bac39",
   "metadata": {},
   "source": [
    "### ðŸ”® Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14bc65-3ebb-4f3c-925b-9a21cefa7e23",
   "metadata": {},
   "source": [
    "We show two cases. In the first case, we use `deel-lip`'s `TauCategoricalCrossentropy` from the `losses` submodule. In the second case, we use this other loss function from `deel-lip`: `MulticlassHKR`.\n",
    "\n",
    "In particular, we will show how these functions can be parametrized to increase the robustness of our predictive models. We will also see that generally, there is a compromise between the robustness and the accuracy of our models (i.e. better robustness generally comes at the price of a decrease in performance).\n",
    "\n",
    "Because we will need to instanciate four times the same model in our examples, we encapsulate the code to create our model in a function for conciseness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359fa47e-86f3-440d-971f-0a49fe7dcdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deel\n",
    "from deel import lip\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "\n",
    "\n",
    "activation=lip.activations.GroupSort2\n",
    "\n",
    "def create_conv_model(name_model):\n",
    "    \"\"\"\n",
    "    A simple convolutional neural network, made to be 1-Lipschitz.\n",
    "    \"\"\"\n",
    "    model= lip.Sequential(\n",
    "        [\n",
    "        Input(shape=X_train.shape[1:]),\n",
    "        \n",
    "        lip.layers.SpectralConv2D(\n",
    "                filters=16,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=activation(),\n",
    "                use_bias=True,\n",
    "                kernel_initializer=\"orthogonal\",\n",
    "            ),\n",
    "        \n",
    "        lip.layers.ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "            \n",
    "        lip.layers.SpectralConv2D(\n",
    "                filters=32,\n",
    "                kernel_size=(3, 3),\n",
    "                activation=activation(),\n",
    "                use_bias=True,\n",
    "                kernel_initializer=\"orthogonal\",\n",
    "            ),\n",
    "        \n",
    "        lip.layers.ScaledL2NormPooling2D(pool_size=(2, 2), data_format=\"channels_last\"),\n",
    "        \n",
    "        Flatten(),\n",
    "        \n",
    "        lip.layers.SpectralDense(\n",
    "                64,\n",
    "                activation=activation(),\n",
    "                use_bias=True,\n",
    "                kernel_initializer=\"orthogonal\",\n",
    "            ),\n",
    "        \n",
    "        lip.layers.SpectralDense(\n",
    "                y_train.shape[-1], activation=None, use_bias=False, kernel_initializer=\"orthogonal\"\n",
    "            ),\n",
    "        ],\n",
    "    \n",
    "        k_coef_lip=1.0,\n",
    "        name=name_model,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4657a28-95c5-4f35-90bd-53a48bb44a5e",
   "metadata": {},
   "source": [
    "#### Cross-entropy loss: `TauCategoricalCrossentropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa1c60-0271-4b76-a0b7-7d2bf89f4550",
   "metadata": {},
   "source": [
    "Similar to the classes we have seen in \"Getting started 1\", the `TauCategoricalCrossentropy` class is similar to its equivalent in `keras`, but it comes with a settable temperature parameter `tau`. This parameter will allow to adjust the robustness of our model. The lower the temperature is, the more robust our model becomes, but it also becomes less accurate.\n",
    "\n",
    "To show the impact of the `tau` on both the performance and robustness of our model, we will train two models on the MNIST dataset. The first model will have a temperature of 100, the second model will have a temperature of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8a3115-ccb4-4ce3-86a2-b3c0a22866b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-temperature model\n",
    "model_1 = create_conv_model(\"cross_entropy_model_1\")\n",
    "\n",
    "temperature_1=100.\n",
    "\n",
    "model_1.compile(\n",
    "    loss=lip.losses.TauCategoricalCrossentropy(tau=temperature_1),\n",
    "    optimizer=Adam(1e-4),\n",
    "    # notice the use of lip.losses.MulticlassKR(), to assess adversarial robustness\n",
    "    metrics=[\"accuracy\", lip.losses.MulticlassKR()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795bf0ad-5495-4b55-aa24-baa6205415b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# low-temperature model\n",
    "model_2 = create_conv_model(\"cross_entropy_model_2\")\n",
    "\n",
    "temperature_2=3.\n",
    "\n",
    "model_2.compile(\n",
    "    loss=lip.losses.TauCategoricalCrossentropy(tau=temperature_2),\n",
    "    optimizer=Adam(1e-4),\n",
    "    metrics=[\"accuracy\", lip.losses.MulticlassKR()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13859a2a-cd4a-4cc5-bc90-f61a07cd22ad",
   "metadata": {},
   "source": [
    "ðŸ’¡ Notice that we use the accuracy metric to measure the performance, and we use the `MulticlassKR` loss to measure adversarial robustness. The latter acts as a proxy of our model's average certificates, **: the higher this measure is, the more robust our model is**.   \n",
    "\n",
    "**ðŸš¨ Note:** *This is true only for 1-Lipschitz neural networks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878ac98-e80a-4d9a-a2a4-ef607e8b8001",
   "metadata": {},
   "source": [
    "We fit both our models and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba315c1-c4df-47aa-ae13-202831555355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the high-temperature model\n",
    "result_1=model_1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    #verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914ac450-e529-406e-8233-79facc4c1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the low-temperature model\n",
    "result_2=model_2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    #verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af5c9695-023b-4fd4-a1f8-d630f3298f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9514\n",
      "Model MulticlassKR: 0.1277\n",
      "Loss' temperature: 100.0\n"
     ]
    }
   ],
   "source": [
    "# metrics for the high-temperature model => performance-oriented \n",
    "print(f\"Model accuracy: {result_1.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Model MulticlassKR: {result_1.history['val_MulticlassKR'][-1]:.4f}\")\n",
    "print(f\"Loss' temperature: {model_1.loss.tau.numpy():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5ed957c-4308-4f75-9a41-dcbc88dcc341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9276\n",
      "Model MulticlassKR: 1.7535\n",
      "Loss' temperature: 3.0\n"
     ]
    }
   ],
   "source": [
    "# metrics for the low-temperature model => robustness-oriented\n",
    "print(f\"Model accuracy: {result_2.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Model MulticlassKR: {result_2.history['val_MulticlassKR'][-1]:.4f}\")\n",
    "print(f\"Loss' temperature: {model_2.loss.tau.numpy():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77b7cd-3d8c-4030-9063-d1e142c47b2f",
   "metadata": {},
   "source": [
    "When decreasing the temperature, we observe an increase in robustness, but a decrease in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8353a6a-4591-45d6-aeb5-eb9c49231f09",
   "metadata": {},
   "source": [
    "#### Hinge-Kantorovichâ€“Rubinstein loss: `MulticlassHKR`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea6020-4102-4956-bbb4-87b169caaf39",
   "metadata": {},
   "source": [
    "We work in the same way as in the previous section. The difference lies in the parameters that control the robustness.\n",
    "\n",
    "We count two of them: `min_margin` (minimal margin) and `alpha` (regularization factor).\n",
    "\n",
    "As will be shown in the following, a higher minimal margin and a lower alpha increases robustness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a964983d-4347-4f57-8489-98e6a05cfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-oriented model\n",
    "model_3 = create_conv_model(\"HKR_model_3\")\n",
    "\n",
    "min_margin_3=1\n",
    "alpha_3=30\n",
    "\n",
    "model_3.compile(\n",
    "    loss=lip.losses.MulticlassHKR(min_margin=min_margin_3,alpha=alpha_3),\n",
    "    optimizer=Adam(1e-4),\n",
    "    metrics=[\"accuracy\", lip.losses.MulticlassKR()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b57e8ad-19fc-4791-8e28-d0e613cbddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# robustness-oriented model\n",
    "model_4 = create_conv_model(\"HKR_model_4\")\n",
    "\n",
    "min_margin_4=3\n",
    "alpha_4=10\n",
    "\n",
    "model_4.compile(\n",
    "    loss=lip.losses.MulticlassHKR(min_margin=min_margin_4,alpha=alpha_4),\n",
    "    optimizer=Adam(1e-4),\n",
    "    metrics=[\"accuracy\", lip.losses.MulticlassKR()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f6ce2-ec2b-4f4d-b282-918d434eb9b3",
   "metadata": {},
   "source": [
    "We fit both our models and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b976968d-4494-4935-8338-06ebd99b6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "235/235 [==============================] - 26s 96ms/step - loss: 10.3240 - accuracy: 0.7246 - MulticlassKR: 0.8375 - val_loss: 3.9340 - val_accuracy: 0.8969 - val_MulticlassKR: 1.3823\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 22s 92ms/step - loss: 2.9818 - accuracy: 0.9019 - MulticlassKR: 1.5695 - val_loss: 1.9182 - val_accuracy: 0.9244 - val_MulticlassKR: 1.7646\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "result_3=model_3.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f19da69-9869-463e-8fac-b7f179e00315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "235/235 [==============================] - 25s 95ms/step - loss: 16.5619 - accuracy: 0.5336 - MulticlassKR: 1.1116 - val_loss: 8.8919 - val_accuracy: 0.7831 - val_MulticlassKR: 2.0187\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 21s 91ms/step - loss: 7.6997 - accuracy: 0.7989 - MulticlassKR: 2.2113 - val_loss: 6.5351 - val_accuracy: 0.8436 - val_MulticlassKR: 2.4005\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "result_4=model_4.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=2,\n",
    "    validation_data=(X_test, y_test),\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bfe7641-f7af-4ac3-9ff4-fca3300448ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9244\n",
      "Model MulticlassKR: 1.7646\n",
      "Loss' minimum margin: 1.0\n",
      "Loss' alpha: 30.0\n"
     ]
    }
   ],
   "source": [
    "# performance-oriented model\n",
    "print(f\"Model accuracy: {result_3.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Model MulticlassKR: {result_3.history['val_MulticlassKR'][-1]:.4f}\")\n",
    "print(f\"Loss' minimum margin: {model_3.loss.min_margin.numpy():.1f}\")\n",
    "print(f\"Loss' alpha: {model_3.loss.alpha.numpy():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ce20cb8-797b-4e03-8600-d0030adfccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.8436\n",
      "Model MulticlassKR: 2.4005\n",
      "Loss' minimum margin: 3.0\n",
      "Loss' alpha: 10.0\n"
     ]
    }
   ],
   "source": [
    "# robustness-oriented model\n",
    "print(f\"Model accuracy: {result_4.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Model MulticlassKR: {result_4.history['val_MulticlassKR'][-1]:.4f}\")\n",
    "print(f\"Loss' minimum margin: {model_4.loss.min_margin.numpy():.1f}\")\n",
    "print(f\"Loss' alpha: {model_4.loss.alpha.numpy():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e39a0d-a145-4a44-85c3-5f67e64ce444",
   "metadata": {},
   "source": [
    "We confirmed experimentally the accuracy-robustness trade-off: a higher minimal margin and a lower alpha increases robustness, but also decreases accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4857464-3ed3-436e-8853-9ed67d0ce5c6",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations\n",
    "You now know how to train adversarially robust 1-Lipschitz neural networks!\n",
    "\n",
    "ðŸ‘“ Interested readers can learn more about the role of loss functions and the accuracy-robustness trade-off which occurs when training adversarially robust 1-Lipschitz neural network in the following paper:   \n",
    " [Pay attention to your loss: understanding misconceptions about 1-Lipschitz neural networks](https://arxiv.org/abs/2104.05097)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
